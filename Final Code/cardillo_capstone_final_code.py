# -*- coding: utf-8 -*-
"""Cardillo_Capstone_Final_Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fFzxePOWQvF5ZvLKbiuQCV1sgKOu1ddO

#Installations
"""

# install and import necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay, classification_report
from sklearn.model_selection import train_test_split
from sklearn import metrics
pd.set_option('display.max_columns', None)
!pip install keplergl --quiet
from keplergl import KeplerGl
!pip install kmodes --quiet
from kmodes.kmodes import KModes

"""# Data Loading + Preprocessing

Please note that feature selection is not used, due to the fact that all questions need to be used to train the model.
"""

#read data into dataframe
data = pd.read_csv("/content/mock_data_rubric.csv")
# checking the features, datatypes, and null values
data.info()

'''
make sure there are no na values present.
normally we would check for duplicates, but since ideally the data is self submitted, there could be duplicate values for the rubric which makes sense realistically
'''
data.dropna(inplace=True)
# dropping the average score column, since it won't be used
data.drop('Average_Score',axis=1, inplace=True)

"""# Exploratory Data Analysis
The purpose of this section is to demonstrate the ways in which we can visualize different aspects of the collegiate scorecard data, which is descriptive information about institutions from the Department of Education. The intention for this is to paint a picture of what institutions that have esports programs look like. This section is just an example of visualizations that can be done, not using actual institutions that have esports programs. This because we will not have a concrete list of these institutions until real data is collected in the fall.
"""

# simulate selecting features from institution data that we would want to bridge with rubric data
selected_features = ['UNITID', 'OPEID', 'INSTNM', 'CITY', 'STABBR', 'ZIP', 'ACCREDAGENCY', 'NUMBRANCH', 'PREDDEG', 'HIGHDEG', 'LOCALE', 'LATITUDE', 'LONGITUDE']
# read school data to pandas dataframe, select above features
school_data = pd.read_csv('/content/Most-Recent-Cohorts-Institution_05192025.csv',low_memory=False)
school_data = school_data[selected_features]

# create a pie chart to show how many schools offer what level of degrees
degrees_granted_dictionary = {
 0.0: 'Non-degree-granting',
 1.0: 'Certificate degree',
 2.0: 'Associate degree',
 3.0: 'Bachelor\'s degree',
 4.0: 'Graduate degree',
}

# map dictionary and get value counts
school_data['HIGHDEG'] = school_data['HIGHDEG'].map(degrees_granted_dictionary)
degree_counts = school_data['HIGHDEG'].value_counts()
# plot pie
plt.pie(degree_counts,autopct='%1.1f%%', labels=degree_counts.index)
plt.title('Highest Degree Awarded')
plt.show()

# create a bar chart to show what kind of institutions predominantly offer which type of degrees
predominant_degree_dictionary = {0: 'Not classified',
                      1: 'Predominantly certificate-degree granting',
                      2: 'Predominantly associate\'s-degree granting',
                      3: 'Predominantly bachelor\'s-degree granting',
                      4: 'Entirely graduate-degree granting'}

# map dictionary to be used in the bar chart
school_data['PREDDEG'] = school_data['PREDDEG'].map(predominant_degree_dictionary)
# create barplot for predominant degree offered
fig, ax = plt.subplots(figsize=(10,10))
sns.barplot(data=school_data['PREDDEG'], palette='Set2', ax=ax, legend=False)
ax.set_xlabel("Count", labelpad=30)
ax.set_ylabel("Predominant Undergraduate Degree Awarded", labelpad=10)
plt.title('Predominant Undergraduate Degree Awarded by Count')
plt.show()

# create barplot for institution accredidations by count
fig, ax = plt.subplots(figsize=(15,15))
sns.barplot(data=school_data['ACCREDAGENCY'], palette='Set2', ax=ax, legend=False)
ax.set_xlabel("Count", labelpad=30)
ax.set_ylabel("Accredidation Agencies by Frequency", labelpad=30)
plt.title('Institution Accredidations by Count')
plt.show()

# create a histogram for the number of institution branches by count (# of institutions that have this # of branches)
fig2, ax2 = plt.subplots(figsize=(10,10))
sns.histplot(data=school_data['NUMBRANCH'], ax=ax2)
ax2.set_xlabel("Number of Branches")
ax2.set_ylabel("Count", labelpad=30)
plt.title("Number of Institution Branches")
plt.show()

# format location data to be used for the kepler gl map
school_map_data = school_data.replace([ np.inf, -np.inf],np.nan).dropna(subset=['LATITUDE','LONGITUDE'])
school_map_data = school_data.loc[:, ['LATITUDE','LONGITUDE']]
school_map_data['LATITUDE'] = school_map_data['LATITUDE'].astype(str)
school_map_data['LONGITUDE'] = school_map_data['LONGITUDE'].astype(str)

# allow widget to display
from google.colab import output
output.enable_custom_widget_manager()

#initialize kepler gl map
school_map = KeplerGl(height=500)
map_data = school_data.iloc[0:1000]
school_map.add_data(map_data, name='school_data')

# display kepler gl map
school_map

"""# Predictive Model
The purpose of this section is to create a predictive model (Random Forest) to classify esports programs into divisions. This model uses mock data due to lack of readily-available data. The intention is to collect real data in the future.
"""

#create a copy of the dataframe to use for clustering later on, so that it does not become encoded
data_cluster = data.copy()
data_cluster = data_cluster.drop(['Division','Program Name'], axis=1)
data_cluster.head()

# initialize label encoder dictionary
encoded_labels = {}
# encode all columns except the column name
columns_to_encode = data.iloc[:,1:]
# for each column that we want to encode, encode and store the encoded labels
for column in columns_to_encode.columns:
  le = LabelEncoder()
  data[column] = le.fit_transform(data[column])
  encoded_labels[column] = dict(zip(le.classes_, le.transform(le.classes_)))
# print encoded labels
print(encoded_labels)

# check to see if the encoding worked
data.head()

# target variable
y = data['Division']
# independent variables
X = data.drop(['Division','Program Name'], axis=1)

# split into testing training and validation sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2)
X_val,X_test,y_val,y_test = train_test_split(X_temp,y_temp,test_size=.5,random_state=42)

# instatiate random forest model, train the model on the training data
rf = RandomForestClassifier()
rf.fit(X_train, y_train)

# generate predictions for test data
y_pred = rf.predict(X_test)

# generate accuracy score, comparing our y predictions to what the test data actually was
accuracy = accuracy_score(y_test,y_pred)
print("Accuracy:", accuracy)

# import necessary modules for k folds cross validation
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

#set the number of k folds, perform k fold cross validation
k = 10
kf = KFold(n_splits=k,shuffle=True,random_state=42)

# evaluate the model on the validation set
y_val_predictions = rf.predict(X_val)
val = accuracy_score(y_val,y_val_predictions)
print(val)

#getting the accuracy, recall, and f1 score to evaluate the model on the test set
y_test_predictions = rf.predict(X_test)
target_names = ['Division 1', 'Division 2', 'Division 3', 'Division 4']
class_report = classification_report(y_test,y_test_predictions,output_dict=True,target_names=target_names)
print(class_report)

# create a heatmap of the classification report
# encoded labels: 'Division 1': np.int64(0), 'Division 2': np.int64(1), 'Division 3': np.int64(2), 'Division 4': np.int64(3)}
report = pd.DataFrame(class_report).transpose()
sns.heatmap(report,annot=True,cbar=False, fmt='.2f')
plt.title('Random Forest Classification Report Metrics')

#create a graph displaying the confusion matrix for the random forest model
confusion = confusion_matrix(y_test,y_test_predictions)
total = confusion.sum() # get sum of test set observations
annot = [[f"{val}\n({val/total:.2%})" for val in row] for row in confusion] # format the annotations
sns.heatmap(confusion, annot=annot, fmt="",cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title('Random Forest Confusion Matrix')
plt.xticks([0.5,1.5,2.5,3.5],['Division 1', 'Division 2', 'Division 3', 'Division 4'])
plt.yticks([0.5,1.5,2.5,3.5],['Division 1', 'Division 2', 'Division 3', 'Division 4'])
plt.show()

# get the accuracy scores for k fold cross validation
k_fold_score = cross_val_score(rf,X_train,y_train, cv=kf,scoring='accuracy')
print("K Fold Accuracy\n" + str(k_fold_score) + "\n")
# f1 score
k_fold_score = cross_val_score(rf,X_train,y_train, cv=kf,scoring='f1_macro')
print("K Fold F1\n" + str(k_fold_score) + "\n")
# roc auc
k_fold_score = cross_val_score(rf,X_train,y_train, cv=kf,scoring='roc_auc_ovr')
print("K Fold ROC OVR\n" + str(k_fold_score) + "\n")

"""# K-Modes Clustering
The purpose of this section is to perform K-Modes clustering on the mock data, to see if the cluster centroids would approximately fall within each division (1-4).
"""

# import kmodes library
from kmodes.kmodes import KModes

# should be sorted in to 4 clusters, to test if the clusters sort into each division properly
km = KModes(n_clusters=4, init='Huang', n_init=5, verbose=1)

# predict and output the clusters
clusters = km.fit_predict(data_cluster)
data_cluster['Cluster'] = clusters

# output the cluster centroids, ideally this should be somewhat of a perfect representation of each division as the centroid
print(km.cluster_centroids_)